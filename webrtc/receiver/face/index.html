<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Baby Monitor – Receiver (Face Landmarks)</title>
  <link rel="icon" href="data:,">
  <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"/>

  <!-- Shared sidebar stylesheet -->
  <link rel="stylesheet" href="/hungryface/webrtc/receiver/shared/sidebar.css" />

  <style>
    :root { color-scheme: dark; }
    html, body {
      margin: 0; height: 100%;
      background: #000; color: #fff;
      font-family: -apple-system, system-ui, Segoe UI, Roboto, sans-serif;
      overflow: hidden;
    }

    /* Fullscreen stage so video + canvas scale together */
    #stage {
      position: fixed; inset: 0;
      width: 100vw; height: 100vh;   /* fallback */
      width: 100dvw; height: 100dvh; /* dynamic viewport */
      width: 100svw; height: 100svh; /* iOS URL bar safe */
      z-index: 1; background: #000;
    }
    #stage > video {
      position: absolute; inset: 0;
      width: 100%; height: 100%;
      object-fit: contain; background: #000;
      z-index: 0; pointer-events: none;
    }
    #stage > canvas {
      position: absolute;
      z-index: 2; background: transparent; display: none;
      pointer-events: none;
    }

    .btn {
      padding: 12px 14px; border-radius: 12px; border: 1px solid rgba(255,255,255,0.6);
      background: transparent; color: #fff; font-size: 16px; font-weight: 600;
      cursor: pointer; box-shadow: none;
      text-shadow: 0 1px 2px rgba(0,0,0,0.6);
    }
    .btn:active { transform: scale(0.99); }

    #controls {
      position: fixed; left: 50%; bottom: 16px; transform: translateX(-50%);
      display: flex; align-items: center; gap: 12px; z-index: 6;
    }
    #enableAudioBtn, #disableAudioBtn { display: none; }

    .overlay {
      position: fixed; inset: 0; display: flex;
      align-items: center; justify-content: center;
      pointer-events: none; z-index: 5;
    }
    .status {
      background: rgba(255,255,255,0.05);
      border: 1px solid rgba(255,255,255,0.1);
      color: #ddd; font-size: 14px; padding: 10px 14px; border-radius: 12px;
      text-align: center; white-space: pre-line; max-width: 90vw;
    }
    .overlay.hidden { display: none; }
  </style>
</head>
<body>
  <!-- ⬇️ Inject shared sidebar markup and behavior -->
  <script type="module">
    (async () => {
      try {
        const res = await fetch('/hungryface/webrtc/receiver/shared/sidebar.html', { cache: 'no-cache' });
        const html = await res.text();
        const wrap = document.createElement('div');
        wrap.innerHTML = html.trim();
        document.body.prepend(...wrap.childNodes);

        await new Promise((resolve, reject) => {
          const s = document.createElement('script');
          s.src = '/hungryface/webrtc/receiver/shared/sidebar.js';
          s.onload = resolve;
          s.onerror = reject;
          document.body.appendChild(s);
        });
      } catch (err) {
        console.error('[Sidebar] failed to load shared assets:', err);
      }
    })();
  </script>

  <div id="controls">
    <button id="enableAudioBtn" class="btn" type="button">Enable audio</button>
    <button id="disableAudioBtn" class="btn" type="button">Disable audio</button>
    <button id="fsBtn" class="btn" type="button">Fullscreen</button>
  </div>

  <div id="stage">
    <video id="remote" autoplay playsinline></video>
    <canvas id="faceCanvas"></canvas>
  </div>

  <div id="overlay" class="overlay">
    <div id="status" class="status">Idle</div>
  </div>

  <script type="module">
    import { DrawingUtils, FaceLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
    import ReceiverCore from "/hungryface/webrtc/receiver/shared/receiver-core.js";

    /* ---------- Constants ---------- */
    const WS_ENDPOINT = "wss://signaling-server-f5gu.onrender.com/ws";
    const room = new URLSearchParams(location.search).get("room") || "Baby";

    const STATUS_HIDE_AFTER_CONNECTED_MS = 10000;

    // MediaPipe assets (keep Tasks version in sync with the import above)
    const TASKS_URL = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
    const WASM_DIR = TASKS_URL + "/wasm";
    // Latest stable Face Landmarker model bundle (served by Google)
    const FACE_MODEL_URL =
      "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task"; // docs "Models" -> Latest. 

    // Video readyState helper
    const HAVE_CURRENT_DATA = 2;

    /* ---------- DOM ---------- */
    const stage = document.getElementById('stage');
    const remoteVideo = document.getElementById('remote');
    const faceCanvas = document.getElementById('faceCanvas');
    const faceCtx = faceCanvas.getContext('2d', { alpha: true });

    const fsBtn = document.getElementById('fsBtn');
    const enableAudioBtn = document.getElementById('enableAudioBtn');
    const disableAudioBtn = document.getElementById('disableAudioBtn');
    const overlay = document.getElementById('overlay');
    const statusEl = document.getElementById('status');

    /* ---------- Status overlay ---------- */
    let hideStatusTimer = null;
    function showStatus(msg) {
      statusEl.textContent = msg;
      overlay.classList.remove('hidden');
      console.log('[STATUS]', msg);
      if (msg === 'Connected') {
        if (hideStatusTimer) clearTimeout(hideStatusTimer);
        hideStatusTimer = setTimeout(() => overlay.classList.add('hidden'), STATUS_HIDE_AFTER_CONNECTED_MS);
      } else {
        if (hideStatusTimer) { clearTimeout(hideStatusTimer); hideStatusTimer = null; }
        overlay.classList.remove('hidden');
      }
    }

    /* ---------- Audio UI ---------- */
    function updateAudioButtons() {
      if (remoteVideo.muted) {
        enableAudioBtn.style.display = 'block';
        disableAudioBtn.style.display = 'none';
      } else {
        enableAudioBtn.style.display = 'none';
        disableAudioBtn.style.display = 'block';
      }
    }
    enableAudioBtn.style.display = 'block';

    async function tryStartMuted() {
      try {
        remoteVideo.muted = true;
        remoteVideo.volume = 1.0;
        await remoteVideo.play();
      } catch (e) { console.warn('autoplay (muted) blocked:', e); }
      updateAudioButtons();
    }
    enableAudioBtn.addEventListener('click', async (e) => {
      e?.stopPropagation?.(); e?.preventDefault?.();
      try { remoteVideo.muted = false; remoteVideo.volume = 1.0; await remoteVideo.play(); }
      catch (err) { console.warn('unmute play blocked:', err); }
      updateAudioButtons();
    });
    disableAudioBtn.addEventListener('click', (e) => {
      e?.stopPropagation?.(); e?.preventDefault?.();
      remoteVideo.muted = true;
      updateAudioButtons();
    });

    /* ---------- Fullscreen ---------- */
    fsBtn.addEventListener('click', async (e) => {
      e.stopPropagation();
      try {
        if (!document.fullscreenElement) {
          await (stage.requestFullscreen?.call(stage) || document.documentElement.requestFullscreen());
          fsBtn.textContent = 'Exit fullscreen';
        } else {
          await document.exitFullscreen();
          fsBtn.textContent = 'Fullscreen';
        }
      } catch (err) { console.warn('fullscreen error', err); }
      beginViewportSettle();
    });
    document.addEventListener('fullscreenchange', () => {
      fsBtn.textContent = document.fullscreenElement ? 'Exit fullscreen' : 'Fullscreen';
      beginViewportSetle();
    });

    remoteVideo.addEventListener('loadedmetadata', () => {
      console.log('[MEDIA] remote meta', remoteVideo.videoWidth, 'x', remoteVideo.videoHeight);
      beginViewportSettle();
    });
    remoteVideo.addEventListener('playing', () => {
      console.log('[MEDIA] remote playing (muted=', remoteVideo.muted, ')');
      updateAudioButtons();
      beginViewportSettle();
    });
    remoteVideo.addEventListener('resize', beginViewportSettle);

    /* ---------- Canvas alignment ---------- */
    function containRect(containerW, containerH, contentW, contentH) {
      const scale = Math.min(containerW / (contentW||1), containerH / (contentH||1));
      const w = Math.round(contentW * scale);
      const h = Math.round(contentH * scale);
      const x = Math.floor((containerW - w) / 2);
      const y = Math.floor((containerH - h) / 2);
      return { left:x, top:y, width:w, height:h };
    }

    function alignCanvasToVideo() {
      const stageBox = stage.getBoundingClientRect();
      const videoBox = remoteVideo.getBoundingClientRect();
      const containerW = Math.round(videoBox.width);
      const containerH = Math.round(videoBox.height);
      const vidW = remoteVideo.videoWidth || 1;
      const vidH = remoteVideo.videoHeight || 1;

      const fit = containRect(containerW, containerH, vidW, vidH);
      const leftCSS = Math.round((videoBox.left - stageBox.left) + fit.left);
      const topCSS = Math.round((videoBox.top - stageBox.top) + fit.top);

      faceCanvas.style.left = leftCSS + 'px';
      faceCanvas.style.top = topCSS + 'px';
      faceCanvas.style.width = fit.width + 'px';
      faceCanvas.style.height = fit.height + 'px';

      const dpr = window.devicePixelRatio || 1;
      const needW = Math.max(1, Math.round(fit.width * dpr));
      const needH = Math.max(1, Math.round(fit.height * dpr));
      if (faceCanvas.width !== needW || faceCanvas.height !== needH) {
        faceCanvas.width = needW;
        faceCanvas.height = needH;
      }
      faceCtx.setTransform(1,0,0,1,0,0);
    }

    let settleRAF = 0, settleUntil = 0;
    function beginViewportSettle(durationMs = 1200) {
      settleUntil = performance.now() + durationMs;
      if (settleRAF) return;
      const tick = () => {
        alignCanvasToVideo();
        if (performance.now() < settleUntil) {
          settleRAF = requestAnimationFrame(tick);
        } else {
          cancelAnimationFrame(settleRAF); settleRAF = 0;
          alignCanvasToVideo();
        }
      };
      tick();
    }
    const vpAlign = () => beginViewportSettle();
    ['resize','orientationchange','scroll'].forEach(ev =>
      window.addEventListener(ev, vpAlign, { passive: true })
    );
    if (window.visualViewport) {
      window.visualViewport.addEventListener('resize', vpAlign, { passive: true });
      window.visualViewport.addEventListener('scroll',  vpAlign, { passive: true });
    }
    document.addEventListener('visibilitychange', () => {
      if (!document.hidden) beginViewportSettle();
    });

    /* ---------- Overlay helpers ---------- */
    function setOverlayVisible(v) {
      faceCanvas.style.display = v ? 'block' : 'none';
      if (v) beginViewportSettle();
      else faceCtx.clearRect(0,0, faceCanvas.width, faceCanvas.height);
    }
    function clearOverlayDrawing() {
      try {
        faceCtx.setTransform(1,0,0,1,0,0);
        faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
      } catch {}
    }

    /* ---------- MediaPipe Face Landmarker (Receiver) ---------- */
    let faceTask = null;
    let faceLoopRunning = false;
    let faceEnabled = true;         // can be toggled via DC if you want
    let faceWhere = 'receiver';     // 'receiver' | 'sender'
    let faceDC = null;              // data channel (optional; drawn if sender streams landmarks)

    async function ensureFaceTask() {
      if (faceTask) return faceTask;
      const vision = await FilesetResolver.forVisionTasks(WASM_DIR);
      faceTask = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: { modelAssetPath: FACE_MODEL_URL, delegate: "GPU" },
        runningMode: "VIDEO",
        numFaces: 1,
        outputFaceBlendshapes: false,
        outputFacialTransformationMatrixes: false
      });
      return faceTask;
    }

    function videoHasFrame(v) {
      return v &&
        v.readyState >= HAVE_CURRENT_DATA &&
        v.videoWidth  > 0 &&
        v.videoHeight > 0;
    }

    function waitForVideoFrame(video) {
      if (videoHasFrame(video)) return Promise.resolve();
      return new Promise((resolve) => {
        const check = () => { if (videoHasFrame(video)) { cleanup(); resolve(); } };
        const cleanup = () => {
          video.removeEventListener('loadedmetadata', check);
          video.removeEventListener('playing', check);
          video.removeEventListener('resize', check);
        };
        video.addEventListener('loadedmetadata', check);
        video.addEventListener('playing', check);
        video.addEventListener('resize', check);
      });
    }

    function drawFaceLandmarks(landmarksArray) {
      if (!faceCanvas.width || !faceCanvas.height) return;

      faceCtx.save();
      faceCtx.clearRect(0,0, faceCanvas.width, faceCanvas.height);

      const utils = new DrawingUtils(faceCtx);
      for (const lm of landmarksArray || []) {
        // Draw connectors if constants exist, else just points
        try { FaceLandmarker.FACE_LANDMARKS_TESSELATION && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_TESSELATION); } catch {}
        try { FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE  && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE); } catch {}
        try { FaceLandmarker.FACE_LANDMARKS_LEFT_EYE   && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_LEFT_EYE); } catch {}
        try { FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW); } catch {}
        try { FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW  && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW); } catch {}
        try { FaceLandmarker.FACE_LANDMARKS_LIPS && utils.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_LIPS); } catch {}
        // Always draw points
        utils.drawLandmarks(lm, { radius: 1.1 });
      }

      faceCtx.restore();
    }

    async function startReceiverFaceLoop() {
      if (faceLoopRunning) return;
      await ensureFaceTask();
      await waitForVideoFrame(remoteVideo);

      faceLoopRunning = true;
      console.log('[FACE][receiver] inference START (receiver)');

      const useRVFC = 'requestVideoFrameCallback' in remoteVideo;

      const onFrame = () => {
        if (!faceLoopRunning || !faceEnabled || faceWhere !== 'receiver') {
          console.log('[FACE][receiver] inference STOP (receiver)');
          return;
        }

        if (!videoHasFrame(remoteVideo)) {
          faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
          scheduleNext();
          return;
        }

        try {
          const ts = performance.now();
          const result = faceTask.detectForVideo(remoteVideo, ts);
          if (result?.faceLandmarks?.length) {
            drawFaceLandmarks(result.faceLandmarks);
            // Optional: stream to sender/debugger via DC
            if (faceDC?.readyState === 'open') {
              faceDC.send(JSON.stringify({ type:'face', landmarks: result.faceLandmarks, ts }));
            }
          } else {
            faceCtx.clearRect(0,0, faceCanvas.width, faceCanvas.height);
          }
        } catch (err) {
          console.warn('[FACE][receiver] detect error (retrying next frame)', err);
        } finally {
          scheduleNext();
        }
      };

      const scheduleNext = () => {
        if (useRVFC) remoteVideo.requestVideoFrameCallback(onFrame);
        else requestAnimationFrame(onFrame);
      };

      scheduleNext();
    }

    function stopReceiverFaceLoop() {
      faceLoopRunning = false;
      faceCtx.clearRect(0,0, faceCanvas.width, faceCanvas.height);
    }

    function applyFaceMode() {
      setOverlayVisible(faceEnabled);
      if (!faceEnabled) { stopReceiverFaceLoop(); return; }
      if (faceWhere === 'receiver') startReceiverFaceLoop();
      else stopReceiverFaceLoop(); // sender mode: draw only what we receive
    }

    /* ---------- Hook up ReceiverCore ---------- */
    const core = new ReceiverCore({
      wsEndpoint: WS_ENDPOINT,
      room,
      onStatus: showStatus,
      // Called when a new RTCPeerConnection is created; set up tracks & DCs here.
      onCreatePC: (pc) => {
        pc.addTransceiver('video', { direction: 'recvonly' });
        pc.addTransceiver('audio', { direction: 'recvonly' });

        // Optional / symmetrical DC for face data
        const ch = pc.createDataChannel('face');
        ch.onopen  = () => console.log('[DC][receiver] face DC open');
        ch.onclose = () => console.log('[DC][receiver] face DC close');
        ch.onerror = (e) => console.warn('[DC][receiver] face DC error', e);
        ch.onmessage = (ev) => {
          try {
            const msg = JSON.parse(ev.data);
            if (msg.type === 'face-mode') {
              faceEnabled = !!msg.enabled;
              faceWhere   = msg.where || 'sender';
              console.log('[FACE][receiver] mode:', { faceEnabled, faceWhere });
              applyFaceMode();
            } else if (msg.type === 'face') {
              if (faceEnabled && faceWhere === 'sender') drawFaceLandmarks(msg.landmarks);
            }
          } catch {}
        };
        faceDC = ch;

        pc.ontrack = (ev) => {
          if (remoteVideo.srcObject !== ev.streams[0]) {
            remoteVideo.srcObject = ev.streams[0];
            tryStartMuted();
            attachStreamEndHandlers(ev.streams[0]); // clear overlay when track stops/mutes
          }
          console.log('[TRACK][receiver] ontrack:', ev.track.kind);
          beginViewportSettle();
        };

        // Keep the status overlay informative
        pc.oniceconnectionstatechange = () => {
          const state = pc.iceConnectionState;
          console.log('[ICE]', state);
          if (state === 'connected' || state === 'completed') {
            showStatus('Connected');
          } else if (state === 'failed') {
            clearOverlayDrawing();
            showStatus('ICE: failed – retrying…');
          } else if (state === 'disconnected') {
            clearOverlayDrawing();
            showStatus('ICE: disconnected');
          } else if (state === 'closed') {
            clearOverlayDrawing();
            showStatus('ICE: closed');
          } else {
            showStatus('ICE: ' + state);
          }
        };
      },
      // When the WS closes entirely, wipe drawings
      onSignalingClosed: () => clearOverlayDrawing(),
    });

    // Clear drawings on common video stoppage states
    ['pause','ended','emptied','stalled','suspend','waiting'].forEach((ev) => {
      remoteVideo.addEventListener(ev, clearOverlayDrawing, { passive: true });
    });

    function attachStreamEndHandlers(stream) {
      if (!stream) return;
      stream.getVideoTracks().forEach((t) => {
        t.addEventListener('ended',  clearOverlayDrawing);
        t.addEventListener('mute',   clearOverlayDrawing);
      });
      stream.addEventListener('removetrack', (e) => {
        if (e.track?.kind === 'video') clearOverlayDrawing();
      });
    }

    window.addEventListener('beforeunload', () => {
      try { core?.stop?.(); } catch {}
      if (hideStatusTimer) clearTimeout(hideStatusTimer);
      stopReceiverFaceLoop();
    });

    // Go!
    core.start();
    setOverlayVisible(true); // show canvas immediately for alignment

    // tiny typo guard used in logs
    function beginViewportSetle(){ beginViewportSettle(); }
  </script>
</body>
</html>
