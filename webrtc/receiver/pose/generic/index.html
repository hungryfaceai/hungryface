<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Baby Monitor – Receiver</title>
  <link rel="icon" href="data:,">
  <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"/>

  <!-- ⬇️ Add shared sidebar stylesheet -->
  <link rel="stylesheet" href="/hungryface/webrtc/receiver/shared/sidebar.css" />

  <style>
    :root { color-scheme: dark; }
    html, body {
      margin: 0; height: 100%;
      background: #000; color: #fff;
      font-family: -apple-system, system-ui, Segoe UI, Roboto, sans-serif;
      overflow: hidden;
    }
    /* Fullscreen wrapper so video+canvas scale together */
    #stage {
      position: fixed; inset: 0;
      width: 100vw; height: 100vh;   /* fallback */
      width: 100dvw; height: 100dvh; /* dynamic viewport */
      width: 100svw; height: 100svh; /* small viewport (iOS URL bar safe) */
      z-index: 1; background: #000;
    }
    #stage > video {
      position: absolute; inset: 0;
      width: 100%; height: 100%;
      object-fit: contain; background: #000;
    }
    #stage > canvas {
      position: absolute;
      z-index: 2; pointer-events: none; background: transparent; display: none;
    }

    .btn {
      padding: 12px 14px; border-radius: 12px; border: 1px solid rgba(255,255,255,0.6);
      background: transparent; color: #fff; font-size: 16px; font-weight: 600;
      cursor: pointer; box-shadow: none;
      text-shadow: 0 1px 2px rgba(0,0,0,0.6);
    }
    .btn:active { transform: scale(0.99); }

    #controls {
      position: fixed; left: 50%; bottom: 16px; transform: translateX(-50%);
      display: flex; align-items: center; gap: 12px; z-index: 6;
    }
    #enableAudioBtn, #disableAudioBtn { display: none; }

    .overlay {
      position: fixed; inset: 0; display: flex;
      align-items: center; justify-content: center;
      pointer-events: none; z-index: 5;
    }
    .status {
      background: rgba(255,255,255,0.05);
      border: 1px solid rgba(255,255,255,0.1);
      color: #ddd; font-size: 14px; padding: 10px 14px; border-radius: 12px;
      text-align: center; white-space: pre-line; max-width: 90vw;
    }
    .overlay.hidden { display: none; }
  </style>
</head>
<body>
  <!-- ⬇️ Inject shared sidebar markup and behavior (kept separate from your code) -->
  <script type="module">
    (async () => {
      try {
        // Load & insert the sidebar HTML (hamburger + nav + invite modal)
        const res = await fetch('/hungryface/webrtc/receiver/shared/sidebar.html', { cache: 'no-cache' });
        const html = await res.text();
        const wrap = document.createElement('div');
        wrap.innerHTML = html.trim();
        document.body.prepend(...wrap.childNodes);

        // Load the sidebar behavior once markup exists
        await new Promise((resolve, reject) => {
          const s = document.createElement('script');
          s.src = '/hungryface/webrtc/receiver/shared/sidebar.js';
          s.onload = resolve;
          s.onerror = reject;
          document.body.appendChild(s);
        });
      } catch (err) {
        console.error('[Sidebar] failed to load shared assets:', err);
      }
    })();
  </script>

  <div id="controls">
    <button id="enableAudioBtn" class="btn" type="button">Enable audio</button>
    <button id="disableAudioBtn" class="btn" type="button">Disable audio</button>
    <button id="fsBtn" class="btn" type="button">Fullscreen</button>
  </div>

  <div id="stage">
    <video id="remote" autoplay playsinline></video>
    <canvas id="poseCanvas"></canvas>
  </div>

  <div id="overlay" class="overlay">
    <div id="status" class="status">Idle</div>
  </div>

  <script type="module">
    import { DrawingUtils, PoseLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
    import { ReceiverCore } from "/hungryface/webrtc/receiver/shared/receiver-core.js";

    /* ---------- Constants ---------- */
    const WS_ENDPOINT = "wss://signaling-server-f5gu.onrender.com/ws";
    const room = new URLSearchParams(location.search).get("room") || "Baby";
    const STATUS_HIDE_AFTER_CONNECTED_MS = 10000;

    // Mediapipe assets (same version as import)
    const TASKS_URL = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
    const WASM_DIR = TASKS_URL + "/wasm";
    const POSE_MODEL_URL =
      "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task";

    // Video readyState helpers
    const HAVE_CURRENT_DATA = 2;

    /* ---------- DOM ---------- */
    const stage = document.getElementById('stage');
    const remoteVideo = document.getElementById('remote');
    const poseCanvas = document.getElementById('poseCanvas');
    const poseCtx = poseCanvas.getContext('2d', { alpha: true });

    const fsBtn = document.getElementById('fsBtn');
    const enableAudioBtn = document.getElementById('enableAudioBtn');
    const disableAudioBtn = document.getElementById('disableAudioBtn');
    const overlay = document.getElementById('overlay');
    const statusEl = document.getElementById('status');

    /* ---------- State ---------- */
    let hideStatusTimer = null;

    // Pose state
    let poseDC = null;
    let poseEnabled = false;
    let poseWhere = 'sender'; // 'sender' | 'receiver'
    let poseTask = null;
    let poseLoopRunning = false;

    /* ---------- Status overlay ---------- */
    function showStatus(msg) {
      statusEl.textContent = msg;
      overlay.classList.remove('hidden');
      console.log('[STATUS]', msg);
      if (msg === 'Connected') {
        if (hideStatusTimer) clearTimeout(hideStatusTimer);
        hideStatusTimer = setTimeout(() => overlay.classList.add('hidden'), STATUS_HIDE_AFTER_CONNECTED_MS);
      } else {
        if (hideStatusTimer) { clearTimeout(hideStatusTimer); hideStatusTimer = null; }
        overlay.classList.remove('hidden');
      }
    }

    /* ---------- Audio UI ---------- */
    function updateAudioButtons() {
      if (remoteVideo.muted) {
        enableAudioBtn.style.display = 'block';
        disableAudioBtn.style.display = 'none';
      } else {
        enableAudioBtn.style.display = 'none';
        disableAudioBtn.style.display = 'block';
      }
    }
    enableAudioBtn.style.display = 'block';

    async function tryStartMuted() {
      try {
        remoteVideo.muted = true;
        remoteVideo.volume = 1.0;
        await remoteVideo.play();
      } catch (e) { console.warn('autoplay (muted) blocked:', e); }
      updateAudioButtons();
    }
    enableAudioBtn.addEventListener('click', async (e) => {
      e?.stopPropagation?.(); e?.preventDefault?.();
      try { remoteVideo.muted = false; remoteVideo.volume = 1.0; await remoteVideo.play(); }
      catch (err) { console.warn('unmute play blocked:', err); }
      updateAudioButtons();
    });
    disableAudioBtn.addEventListener('click', (e) => {
      e?.stopPropagation?.(); e?.preventDefault?.();
      remoteVideo.muted = true;
      updateAudioButtons();
    });

    /* ---------- Fullscreen ---------- */
    fsBtn.addEventListener('click', async (e) => {
      e.stopPropagation();
      try {
        if (!document.fullscreenElement) {
          await (stage.requestFullscreen?.call(stage) || document.documentElement.requestFullscreen());
          fsBtn.textContent = 'Exit fullscreen';
        } else {
          await document.exitFullscreen();
          fsBtn.textContent = 'Fullscreen';
        }
      } catch (err) { console.warn('fullscreen error', err); }
      beginViewportSettle();
    });
    document.addEventListener('fullscreenchange', () => {
      fsBtn.textContent = document.fullscreenElement ? 'Exit fullscreen' : 'Fullscreen';
      beginViewportSetle();
    });

    remoteVideo.addEventListener('loadedmetadata', () => {
      console.log('[MEDIA] remote meta', remoteVideo.videoWidth, 'x', remoteVideo.videoHeight);
      beginViewportSettle();
    });
    remoteVideo.addEventListener('playing', () => {
      console.log('[MEDIA] remote playing (muted=', remoteVideo.muted, ')');
      updateAudioButtons();
      beginViewportSettle();
    });
    // Handle rotation / track size changes from sender
    remoteVideo.addEventListener('resize', beginViewportSettle);

    /* ---------- Canvas alignment ---------- */
    function containRect(containerW, containerH, contentW, contentH) {
      const scale = Math.min(containerW / (contentW||1), containerH / (contentH||1));
      const w = Math.round(contentW * scale);
      const h = Math.round(contentH * scale);
      const x = Math.floor((containerW - w) / 2);
      const y = Math.floor((containerH - h) / 2);
      return { left:x, top:y, width:w, height:h };
    }

    function alignCanvasToVideo() {
      const stageBox = stage.getBoundingClientRect();
      const videoBox = remoteVideo.getBoundingClientRect();
      const containerW = Math.round(videoBox.width);
      const containerH = Math.round(videoBox.height);
      const vidW = remoteVideo.videoWidth || 1;
      const vidH = remoteVideo.videoHeight || 1;

      const fit = containRect(containerW, containerH, vidW, vidH);
      const leftCSS = Math.round((videoBox.left - stageBox.left) + fit.left);
      const topCSS = Math.round((videoBox.top - stageBox.top) + fit.top);

      poseCanvas.style.left = leftCSS + 'px';
      poseCanvas.style.top = topCSS + 'px';
      poseCanvas.style.width = fit.width + 'px';
      poseCanvas.style.height = fit.height + 'px';

      const dpr = window.devicePixelRatio || 1;
      const needW = Math.max(1, Math.round(fit.width * dpr));
      const needH = Math.max(1, Math.round(fit.height * dpr));
      if (poseCanvas.width !== needW || poseCanvas.height !== needH) {
        poseCanvas.width = needW;
        poseCanvas.height = needH;
      }
      poseCtx.setTransform(1,0,0,1,0,0);
    }

    let settleRAF = 0, settleUntil = 0;
    function beginViewportSettle(durationMs = 1200) {
      settleUntil = performance.now() + durationMs;
      if (settleRAF) return;
      const tick = () => {
        alignCanvasToVideo();
        if (performance.now() < settleUntil) {
          settleRAF = requestAnimationFrame(tick);
        } else {
          cancelAnimationFrame(settleRAF); settleRAF = 0;
          alignCanvasToVideo();
        }
      };
      tick();
    }
    const vpAlign = () => beginViewportSettle();
    ['resize','orientationchange','scroll'].forEach(ev =>
      window.addEventListener(ev, vpAlign, { passive: true })
    );
    if (window.visualViewport) {
      window.visualViewport.addEventListener('resize', vpAlign, { passive: true });
      window.visualViewport.addEventListener('scroll',  vpAlign, { passive: true });
    }
    document.addEventListener('visibilitychange', () => {
      if (!document.hidden) beginViewportSettle();
    });

    /* ---------- Overlay helpers ---------- */
    function setOverlayVisible(v) {
      poseCanvas.style.display = v ? 'block' : 'none';
      if (v) beginViewportSettle();
      else poseCtx.clearRect(0,0, poseCanvas.width, poseCanvas.height);
    }

    function drawPoseLandmarks(landmarks) {
      if (!landmarks?.length || !poseCanvas.width || !poseCanvas.height) return;
      poseCtx.save();
      poseCtx.clearRect(0,0, poseCanvas.width, poseCanvas.height);
      const utils = new DrawingUtils(poseCtx);
      for (const lm of landmarks) {
        utils.drawLandmarks(lm, {
          radius: (data) => DrawingUtils.lerp((data.from && data.from.z) ?? 0, -0.15, 0.1, 5, 1)
        });
        utils.drawConnectors(lm, PoseLandmarker.POSE_CONNECTIONS);
      }
      poseCtx.restore();
    }

    /* ---------- Central canvas clear helper ---------- */
    function clearOverlayDrawing() {
      try {
        poseCtx.setTransform(1,0,0,1,0,0);
        poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
      } catch {}
    }

    /* ---------- attach track/stream end handlers ---------- */
    function attachStreamEndHandlers(stream) {
      if (!stream) return;
      // Clear if any video track ends/mutes or is removed
      stream.getVideoTracks().forEach((t) => {
        t.addEventListener('ended',  clearOverlayDrawing);
        t.addEventListener('mute',   clearOverlayDrawing);
      });
      stream.addEventListener('removetrack', (e) => {
        if (e.track?.kind === 'video') clearOverlayDrawing();
      });
    }

    /* ---------- MediaPipe on RECEIVER ---------- */
    async function ensurePoseTask() {
      if (poseTask) return poseTask;
      const vision = await FilesetResolver.forVisionTasks(WASM_DIR);
      poseTask = await PoseLandmarker.createFromOptions(vision, {
        baseOptions: { modelAssetPath: POSE_MODEL_URL, delegate: "GPU" },
        runningMode: "VIDEO",
        numPoses: 1
      });
      return poseTask;
    }

    function videoHasFrame(v) {
      return v &&
        v.readyState >= HAVE_CURRENT_DATA &&
        v.videoWidth  > 0 &&
        v.videoHeight > 0;
    }

    function waitForVideoFrame(video) {
      if (videoHasFrame(video)) return Promise.resolve();
      return new Promise((resolve) => {
        const check = () => { if (videoHasFrame(video)) { cleanup(); resolve(); } };
        const cleanup = () => {
          video.removeEventListener('loadedmetadata', check);
          video.removeEventListener('playing', check);
          video.removeEventListener('resize', check);
        };
        video.addEventListener('loadedmetadata', check);
        video.addEventListener('playing', check);
        video.addEventListener('resize', check);
      });
    }

    async function startReceiverPoseLoop() {
      if (poseLoopRunning) return;
      await ensurePoseTask();
      await waitForVideoFrame(remoteVideo);

      poseLoopRunning = true;
      console.log('[POSE][receiver] inference START (receiver)');

      const useRVFC = 'requestVideoFrameCallback' in remoteVideo;

      const onFrame = () => {
        if (!poseLoopRunning || !poseEnabled || poseWhere !== 'receiver') {
          console.log('[POSE][receiver] inference STOP (receiver)');
          return;
        }

        if (!videoHasFrame(remoteVideo)) {
          poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
          scheduleNext();
          return;
        }

        try {
          const ts = performance.now();
          const result = poseTask.detectForVideo(remoteVideo, ts);
          if (result?.landmarks?.length) {
            drawPoseLandmarks(result.landmarks);
            // send landmarks back to sender when running on receiver
            if (poseDC?.readyState === 'open') {
              poseDC.send(JSON.stringify({ type:'pose', landmarks: result.landmarks, ts }));
            }
          } else {
            poseCtx.clearRect(0,0, poseCanvas.width, poseCanvas.height);
          }
        } catch (err) {
          console.warn('[POSE][receiver] detect error (retrying next frame)', err);
        } finally {
          scheduleNext();
        }
      };

      const scheduleNext = () => {
        if (useRVFC) remoteVideo.requestVideoFrameCallback(onFrame);
        else requestAnimationFrame(onFrame);
      };

      scheduleNext();
    }

    function stopReceiverPoseLoop() {
      poseLoopRunning = false;
      poseCtx.clearRect(0,0, poseCanvas.width, poseCanvas.height);
    }

    function applyPoseMode() {
      setOverlayVisible(poseEnabled);
      if (!poseEnabled) { stopReceiverPoseLoop(); return; }
      if (poseWhere === 'receiver') startReceiverPoseLoop();
      else stopReceiverPoseLoop(); // sender mode: draw only what we receive
    }

    /* ---------- Shared WebRTC core (signaling + PC) ---------- */
    const core = new ReceiverCore({
      wsEndpoint: WS_ENDPOINT,
      room,

      onStatus: showStatus,

      onStream: (stream) => {
        if (remoteVideo.srcObject !== stream) {
          remoteVideo.srcObject = stream;
          tryStartMuted();
          attachStreamEndHandlers(stream);
        }
      },

      onIceState: (state) => {
        if (state === 'failed' || state === 'disconnected' || state === 'closed') {
          clearOverlayDrawing();
        }
      },

      onBye: () => {
        clearOverlayDrawing();
      },

      onCreatePC: (pc) => {
        // Pose DC (receiver initiates; bidirectional)
        const ch = pc.createDataChannel('pose');
        ch.onopen = () => { console.log('[DC][receiver] pose DC open'); };
        ch.onclose = () => { console.log('[DC][receiver] pose DC close'); };
        ch.onerror = (e) => { console.warn('[DC][receiver] pose DC error', e); };
        ch.onmessage = (ev) => {
          try {
            const msg = JSON.parse(ev.data);
            if (msg.type === 'pose-mode') {
              poseEnabled = !!msg.enabled;
              poseWhere   = msg.where || 'sender';
              console.log('[POSE][receiver] mode:', { poseEnabled, poseWhere });
              applyPoseMode();
            } else if (msg.type === 'pose') {
              if (poseEnabled && poseWhere === 'sender') drawPoseLandmarks(msg.landmarks);
            }
          } catch {}
        };
        poseDC = ch;
      },

      onDataChannel: (e) => {
        // Leave inbound DCs open; log for debugging
        console.log('[DC][receiver] inbound datachannel (leaving open):', e.channel?.label);
      }
    });

    core.start().catch(console.warn);

    /* ---------- NEW: clear overlay on video element stop-like states ---------- */
    ['pause','ended','emptied','stalled','suspend','waiting'].forEach((ev) => {
      remoteVideo.addEventListener(ev, clearOverlayDrawing, { passive: true });
    });

    window.addEventListener('beforeunload', () => {
      try { core.close(); } catch {}
      if (hideStatusTimer) clearTimeout(hideStatusTimer);
      stopReceiverPoseLoop();
    });

    // Go! (ReceiverCore started above)

    // tiny typo guard used in logs
    function beginViewportSetle(){ beginViewportSettle(); }
  </script>
</body>
</html>
