<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Live Cat vs Dog Audio Classifier</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <h1>ğŸ¶ğŸ± Live Audio Classifier</h1>
  <button id="start-btn">ğŸ™ï¸ Start Listening</button>
  <p id="status">Idle</p>
  <div id="prediction">Prediction: <strong>--</strong></div>
  <div id="bars">
    <div class="bar"><label>Dog</label><progress id="dog-bar" max="1" value="0"></progress></div>
    <div class="bar"><label>Cat</label><progress id="cat-bar" max="1" value="0"></progress></div>
  </div>

  <script type="module">
    const MODEL_URL = '../audio2/model_tfjs_audio/model.json';
    const SAMPLE_RATE = 16000;
    const WINDOW_SIZE = SAMPLE_RATE * 1; // 1s window = 16000 samples
    const HOP_INTERVAL = 500; // ms between predictions
    const BUFFER_SIZE = WINDOW_SIZE * 2; // Circular buffer twice the window size

    const classNames = ['dog', 'cat'];
    let audioBuffer = new Float32Array(BUFFER_SIZE);
    let writeIndex = 0;
    let listening = false;
    let model;

    async function loadModel() {
      model = await tf.loadGraphModel(MODEL_URL);
      console.log("âœ… Model loaded");
    }

    async function startListening() {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });
      const source = audioCtx.createMediaStreamSource(stream);

      const processor = audioCtx.createScriptProcessor(4096, 1, 1);
      source.connect(processor);
      processor.connect(audioCtx.destination);

      processor.onaudioprocess = (e) => {
        const input = e.inputBuffer.getChannelData(0);
        for (let i = 0; i < input.length; i++) {
          audioBuffer[writeIndex] = input[i];
          writeIndex = (writeIndex + 1) % BUFFER_SIZE;
        }
      };

      setInterval(runInference, HOP_INTERVAL);
      listening = true;
      document.getElementById('status').innerText = 'Listening...';
    }

    async function runInference() {
      if (!listening || !model) return;

      const latest = new Float32Array(WINDOW_SIZE);
      for (let i = 0; i < WINDOW_SIZE; i++) {
        const idx = (writeIndex + i) % BUFFER_SIZE;
        latest[i] = audioBuffer[idx];
      }

      const inputTensor = tf.tensor(latest);
      const inputName = model.inputs[0].name.split(':')[0];

      try {
        const result = model.execute({ [inputName]: inputTensor });
        let probs;

        if (Array.isArray(result)) {
          probs = await result[0].data();
        } else if (result instanceof tf.Tensor) {
          probs = await result.data();
        } else if (typeof result === 'object') {
          probs = await result['probs'].data();
        } else {
          throw new Error("Unknown model output format");
        }

        const predictedIndex = probs.indexOf(Math.max(...probs));
        updateUI(predictedIndex, probs);

      } catch (err) {
        console.error("âŒ Prediction error:", err);
      }
    }

    function updateUI(predictedIndex, probs) {
      document.querySelector('#prediction strong').innerText = classNames[predictedIndex];
      document.getElementById('dog-bar').value = probs[0];
      document.getElementById('cat-bar').value = probs[1];
    }

    document.getElementById('start-btn').addEventListener('click', async () => {
      document.getElementById('status').innerText = 'Loading model...';
      await loadModel();
      startListening();
    });
  </script>
</body>
</html>
