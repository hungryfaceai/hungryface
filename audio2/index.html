<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Cat vs Dog Audio Classifier</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/meyda/dist/web/meyda.min.js"></script>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
    }
    canvas {
      border: 1px solid black;
      margin-top: 10px;
    }
  </style>
  <link rel="icon" href="data:,">
</head>
<body>
  <h1>Cat vs Dog Audio Classifier</h1>
  <input type="file" id="audio-upload" accept=".wav" />
  <p id="status">Please upload a .wav file</p>
  <p id="prediction"></p>
  <canvas id="spectrogram" width="800" height="256"></canvas>

  <script type="module">
    window.addEventListener('DOMContentLoaded', async () => {
      const MODEL_URL = './tfjs_model_audio/model.json';
      const classNames = ['dog', 'cat'];
      const canvas = document.getElementById('spectrogram');
      const ctx = canvas.getContext('2d');

      // âœ… In-browser resampling using Web Audio API
      async function resampleInBrowser(file, targetSampleRate = 16000) {
        const originalCtx = new (window.AudioContext || window.webkitAudioContext)();
        const arrayBuffer = await file.arrayBuffer();
        const decoded = await originalCtx.decodeAudioData(arrayBuffer);

        if (decoded.sampleRate === targetSampleRate) {
          return decoded.getChannelData(0); // already 16kHz
        }

        const duration = decoded.duration;
        const offlineCtx = new OfflineAudioContext(1, duration * targetSampleRate, targetSampleRate);
        const source = offlineCtx.createBufferSource();
        source.buffer = decoded;
        source.connect(offlineCtx.destination);
        source.start(0);
        const rendered = await offlineCtx.startRendering();
        return rendered.getChannelData(0); // Float32Array
      }

      function drawSpectrogram(waveform, sampleRate = 16000) {
        const bufferSize = 512;
        const hopSize = 256;
        const melBands = 64;
        const frames = Math.floor((waveform.length - bufferSize) / hopSize);
        const spectrogramData = [];

        const analyzer = Meyda.createMeydaAnalyzer({
          audioContext: null,
          bufferSize: bufferSize,
          sampleRate: sampleRate,
          windowingFunction: 'hamming',
          featureExtractors: ['mfcc'],
          numberOfMFCCCoefficients: melBands
        });

        for (let i = 0; i < frames; i++) {
          const start = i * hopSize;
          const frame = waveform.slice(start, start + bufferSize);
          const spectrum = analyzer.extract('mfcc', frame);
          spectrogramData.push(spectrum || new Array(melBands).fill(0));
        }

        const width = canvas.width;
        const height = canvas.height;
        const img = ctx.createImageData(width, height);
        const pixelsPerFrame = Math.floor(width / spectrogramData.length);

        for (let x = 0; x < spectrogramData.length; x++) {
          for (let y = 0; y < melBands; y++) {
            const val = spectrogramData[x][y];
            const intensity = Math.min(255, Math.max(0, (val + 100) * 2));
            const px = x * pixelsPerFrame;
            const py = height - Math.floor((y / melBands) * height);
            for (let dx = 0; dx < pixelsPerFrame; dx++) {
              const index = ((py * width) + px + dx) * 4;
              img.data[index] = intensity;
              img.data[index + 1] = intensity / 2;
              img.data[index + 2] = 0;
              img.data[index + 3] = 255;
            }
          }
        }

        ctx.putImageData(img, 0, 0);
      }

      document.getElementById('audio-upload').addEventListener('change', async (e) => {
        const file = e.target.files[0];
        if (!file) return;

        document.getElementById('status').innerText = 'Processing audio...';

        try {
          const waveform = await resampleInBrowser(file);
          drawSpectrogram(waveform);

          const model = await tf.loadGraphModel(MODEL_URL);
          const audioTensor = tf.tensor(waveform).expandDims(0);
          const result = model.predict(audioTensor);
          const probs = await result.data();

          const prediction = classNames[probs.indexOf(Math.max(...probs))];
          document.getElementById('prediction').innerText =
            `Prediction: ${prediction} (dog: ${probs[0].toFixed(3)}, cat: ${probs[1].toFixed(3)})`;
          document.getElementById('status').innerText = 'Done';
        } catch (err) {
